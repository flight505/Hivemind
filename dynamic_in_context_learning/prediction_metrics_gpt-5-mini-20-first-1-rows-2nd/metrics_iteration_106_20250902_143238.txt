PREDICTIVE METRICS - ITERATION 106
============================================================

Executive summary — immediate takeaways & top priorities (0–72h)
- What happened (short): A new brittle failure surfaced on a single-record batch (n==1):
  - 0134_01: sum_spend = 744, top1_channel = ShoppingMall (730), top1_share ≈ 0.98, spend_entropy ≈ 0 — model predicted False but ground truth True. This is a single-feature-concentration false negative (the model treated high ShoppingMall dominance as negative or under-expressed uncertainty).
  - This is complementary to earlier brittle modes (all_zero false-negatives and extreme single-channel false-positives). All are small-n, slice-fragile failures tied to concentration and channel-specific sign/priors.
- Immediate implication: n==1 (and small-n) auto-decisions must be blocked for fragile flags until consensus/uncertainty checks pass. We must treat concentration as channel-aware (top1_channel matters) and inflate SE / rely more on pooled priors when slice context is weak. Add this record to canaries and fast-audit queue.
- Top priorities (0–72h):
  1. Enforce n==1 stopgap gating for fragile_flag (all_zero OR top1_share ≥ TOP1_CONC_THRESHOLD OR sum_spend ≥ ABS_SPEND_HIGH OR feature_dom_fraction ≥ FEATURE_DOMINANCE_THRESH), but require channel-aware context_score and GLM/ensemble consensus to auto-accept.
  2. Persist new channel-aware context scores and flags in provenance: top1_channel, top1_channel_context_score, top1_channel_pos_frac, in addition to previous flags.
  3. Inflate SE for concentration-by-channel slices (add var_concentration_by_channel) and add dynamic SE floors.
  4. Retrain calibrator & GLM_fallback with explicit interactions including top1_channel × top1_share × sum_spend × Age_bucket × HomePlanet × CryoSleep; upweight contradictory examples ×3–5 and shadow-run ≥14 days.
  5. Add canaries (0126_01, 0127_01, 0133_01, 0133_02, 0134_01) and block auto-decisions for them until validations pass.

1) What specific patterns caused this error?
- Primary pattern for 0134_01:
  - High single-channel concentration: top1_channel = ShoppingMall, top1_spend = 730, sum_spend = 744, top1_share ≈ 0.98 → concentration_flag triggered.
  - Moderate absolute spend (744) — just below prior ABS_SPEND_HIGH=800 — so absolute-spend gating would not have triggered, but concentration did.
  - The model apparently applied a negative (or non-supportive) signal to ShoppingMall-dominant records in the learned weights or calibrator (channel-specific sign), or the calibrator under-expressed uncertainty for rare concentrated channel patterns, causing a confident False.
- Contributing/systemic issues:
  - No channel-aware concentration context score: concentration_flag was present but top1_channel context (how often ShoppingMall-dominant records are transported) was not used sufficiently.
  - SE model lacked var_concentration_by_channel; se_combined was too small for this fragile slice.
  - Pooled priors were not stratified by top1_channel × concentration; N0 for blending was not large enough for rare concentration slices.
  - Feature transforms / capping: per-channel logit caps and/or winsorization were not applied uniformly; single-channel dominance could yield excessive signed contributions.
  - Calibrator training data lacked sufficient contradictory examples (ShoppingMall-dominant transported cases) and was underweighted for such slices.

2) How should decision rules be modified to prevent recurrence?
- Make concentration-by-channel a first-class gating feature:
  - new flag: concentration_by_channel_flag = (top1_share ≥ TOP1_CONC_THRESHOLD AND top1_channel ∈ known_channels)
  - compute top1_channel_context_score = P(transported | top1_channel = X, top1_share ≥ TOP1_CONC_THRESHOLD, HomePlanet, Age_bucket, CryoSleep)
- Revised gating policy (n==1):
  - If any fragile_flag True (all_zero_flag OR concentration_by_channel_flag OR abs_spend_flag OR feature_dom_fraction ≥ FEATURE_DOMINANCE_THRESH):
    - Allow auto-decision ONLY IF ALL:
      - pattern_context_score ≥ Z_high (channel-aware for concentration)
      - N_samples_in_slice ≥ N_min (pattern-dependent, channel-aware)
      - ensemble_agreement ≥ A_high
      - GLM_fallback_agrees
      - se_combined ≤ SE_accept
    - Else → priority_audit (block auto-decision)
  - For concentration_by_channel_flag specifically:
    - Use top1_channel_context_score and top1_channel_pos_frac with N_min_conc (e.g., 25) to determine sufficiency.
- Example initial sweepable thresholds:
  - TOP1_CONC_THRESHOLD = 0.70
  - FEATURE_DOMINANCE_THRESH = 0.60
  - ABS_SPEND_HIGH = 800
  - Z_high = 0.80
  - N_min_conc = 25
  - A_high = 0.995
  - SE_accept = 0.06
- Pseudocode (simplified):
  - If n_batch == 1:
      If any(fragile_flag):
        Determine pattern_context_score (channel-aware if concentration)
        If pattern_context_score ≥ Z_high AND N_slice ≥ N_min_pattern AND ensemble_agreement ≥ A_high AND GLM_fallback_agrees AND se_combined ≤ SE_accept:
          allow_auto_decision()
        Else:
          priority_audit()
      Else:
        normal_gating()

3) What new insights does this error reveal about passenger transport patterns?
- Concentration is channel- and context-dependent:
  - High top1_share does not universally mean positive or negative; direction and strength depend on which channel is dominant (ShoppingMall vs RoomService vs FoodCourt), Age, HomePlanet, Destination, CryoSleep and cabin class.
- Moderate absolute spend with very high concentration is fragile: a single-channel signal can dominate model logits and flip decisions, especially in small-n batches.
- Training data imbalance: some channel-dominant transported examples are rare, causing the model/calibrator to learn a biased sign or under-express uncertainty.
- Therefore pooled priors and variance must be channel-aware for concentration slices.

4) How should confidence be recalibrated for more accurate batch predictions?
- Add concentration-by-channel variance term and dynamic SE floors:
  - var_conc_by_channel = κ_conc_chan * (1 − top1_channel_context_score) * (top1_share^2) * log1p(sum_spend)
  - var_feature_dom (existing) + var_conc_by_channel should inflate se_combined for ShoppingMall-dominant records when context weak.
- Combine variance terms:
  - var_combined = var_base + var_dispersion + var_spend_scale + var_all_zero + var_missingness + var_concentration + var_abs_spend + var_feature_dom + var_conc_by_channel
  - se_combined = sqrt(max(var_combined, base_min_se(context)^2))
- Dynamic SE floors (initial defaults, sweepable):
  - concentration_by_channel weak-context → se_floor = 0.20–0.35
  - concentration_by_channel strong-context → se_floor = 0.06–0.10
- Calibrator outputs: p10/p50/p90 (quantiles) + sd; gate on quantiles:
  - Only allow auto-accept when p10 > accept_thres or p90 < reject_thres (for confident cases). For fragile patterns, require narrower quantile spread.
- Example κ defaults:
  - κ_conc_chan = 0.06 (sweepable), κ_dom = 0.07, κ_abs = 0.05

5) What adjustments are needed for better consistency across batch predictions?
- Standardize feature computation & semantics across scorer, pooled_prior, GLM_fallback, calibrator:
  - top1_channel, top1_spend, top1_share (NULL for all_zero), spend_entropy_norm, num_nonzero_channels, feature_dom_fraction.
  - top1_channel_context_score and top1_channel_pos_frac persisted daily.
- Pooled priors must be channel-aware:
  - μ_conc_channel_demo: mean transport probability for (top1_channel = X, top1_share_bucket, HomePlanet, Age_bucket, CryoSleep).
  - Blend: w_data = n / (n + N0_pattern_channel), where N0_pattern_channel is larger for fragile channel-concentration slices (e.g., 25–100).
- Per-feature contribution caps:
  - Cap per-channel logit contribution to avoid single-channel dominance overpowering other signals (e.g., max_contrib = 3.0–4.0 logits). Alternatively use monotonic constraints with sign priors per channel.
- Retrain/specs:
  - Retrain calibrator and GLM_fallback with explicit top1_channel interactions; upweight contradictory examples (concentrated transported or concentrated non-transported) ×3–5.
  - Shadow-run ≥ 14 days; require targeted slice improvements without global calibration regressions.
- Provenance:
  - Persist channel-aware context scores, N_slice per channel, pooled priors and τ weights.

6) How can the metrics be improved to handle edge cases like this one?
- New slice monitors & alerts:
  - concentration_by_channel_by_ctx (top1_channel × top1_share_bucket × HomePlanet × Age_bucket): track ECE, Brier, FP, FN, contradiction_count, n==1_auto_accept_rate.
  - per-channel top1_sign_consistency (fraction positive when top1_channel=X & top1_share>threshold).
  - Global and per-slice n==1_auto_accept_contradiction_rate.
- Canaries & active learning:
  - Add 0134_01 to canaries (plus 0126_01, 0127_01, 0133_01, 0133_02). Block auto-decision for canaries until validated.
  - Seed active-label queue with concentration_by_channel × HomePlanet × Age combos to get corrected labels faster.
- Retrain plan:
  - Calibrator:
    - Inputs include model_logit, ensemble_agreement, all_zero_flag, concentration_by_channel_flag, top1_channel, top1_share, spend_entropy_norm, num_nonzero_channels, missingness_profile, top1_channel_context_score, abs_spend_context_score, CryoSleep, Age_bucket, HomePlanet.
    - Targets: quantile outputs p10/p50/p90 and sd.
    - Loss: quantile loss + ECE penalty + upweight contradictions ×3–5.
  - GLM_fallback:
    - Interactions: top1_channel × top1_share_bucket × sum_spend_bucket × Age_bucket × HomePlanet × CryoSleep.
    - Elastic-net regularization and sign-stability checks; shrink rare interactions.
- CI & Tests:
  - Unit tests ensuring gating triggers for concentration_by_channel records with n==1 unless context_score≥Z_high & GLM agrees.
  - Tests that se_combined increases for concentration_by_channel slices and calibrator quantile spread enlarges for weak-context slices.
  - Tests that per-channel pooled priors are applied and blending weight τ respects N0 for fragile channels.

Complete updated predictive metrics report — actionable components

A. New / updated feature definitions (versioned v→v+1)
- sum_spend = RoomService + FoodCourt + ShoppingMall + Spa + VRDeck (raw & log1p).
- all_zero_flag = (sum_spend == 0 AND num_nonzero_channels == 0)
- top1_channel, top1_spend, top1_share:
  - If all_zero_flag → top1_share = NULL, concentration_type = 'all_zero'
  - Else → identify top1_channel and top1_share = top1_spend / max(1, sum_spend)
- concentration_by_channel_flag = (top1_share ≥ TOP1_CONC_THRESHOLD) with TOP1_CONC_THRESHOLD = 0.70
- top1_share_bucket = bucket(top1_share) e.g., [0.0–0.25, 0.25–0.5, 0.5–0.7, 0.7–0.9, 0.9–1.0]
- top1_channel_context_score = smoothed P(transported | top1_channel, top1_share_bucket, HomePlanet, Age_bucket, CryoSleep) using Bayesian smoothing (α).
- top1_channel_pos_frac = empirical fraction transported for same slice (no smoothing).
- concentration_channel_sign_flip_flag = True if top1_channel_pos_frac significantly differs from global pos_frac (>Δ_sign_flip, e.g., 0.20) and N_slice small.
- spend_entropy_norm = normalized Shannon entropy across channel spends (null when all_zero)
- feature_dom_fraction = per-feature logit_contrib_abs / total_logit_abs_contrib (computed on calibrated logits). If ≥ FEATURE_DOMINANCE_THRESH = 0.60 → single_feature_influence_flag = True
- missingness_profile, missingness_count as before

B. Pooled priors extension (channel-aware)
- μ_conc_channel_demo: pooled prior mean for records with top1_channel = X and top1_share_bucket, stratified by (HomePlanet × Age_bucket × CryoSleep × Cabin × VIP)
- Blend rule (channel-aware):
  - τ_channel = N_slice_channel / (N_slice_channel + N0_channel)
  - N0_channel larger for fragile concentration slices (e.g., ShoppingMall concentration N0 = 50–200; default N0 = 3–10)
  - μ_blended_channel = τ_channel*μ_slice + (1−τ_channel)*μ_global_channel
  - p_final = w_data*p_model + (1−w_data)*μ_blended_channel with w_data = n / (n + N0_channel)

C. Direction-aware logit shifts (pattern & channel treatment)
- Compute channel_shift_frac per record:
  - channel_shift_frac = clamp(base_channel_shift + w_chan_ctx*(top1_channel_context_score − 0.5)*2 + w_chan_age*age_norm, −0.5, 0.5)
  - For fragile channel slices, damp by τ_channel (small N_slice reduces effective shift).
- Apply shift as additive logit offset only when context strong or N_slice sufficient.

D. Variance / SE model (explicit, add concentration-by-channel)
- New variance terms (examples; sweepable κ):
  - var_conc_by_channel = κ_conc_chan * (1 − top1_channel_context_score) * (top1_share^2) * log1p(sum_spend)
  - var_all_zero = κ_zero * (1 − all_zero_context_score) * sqrt(1 + num_imputed_features)
  - var_missingness = κ_miss * missingness_count * novelty_scale
  - var_abs_spend = κ_abs * log1p(sum_spend)/scale * (1 − abs_spend_context_score)
  - var_feature_dom = κ_dom * max(0, feature_dom_fraction − FEATURE_DOMINANCE_BASE)
  - var_spend_scale = κ_scale * log1p(sum_spend)
- Combine:
  - var_combined = var_base + var_dispersion + var_spend_scale + var_all_zero + var_missingness + var_concentration + var_abs_spend + var_feature_dom + var_conc_by_channel
  - se_combined = sqrt(max(var_combined, base_min_se(context)^2))
- Example κ defaults:
  - κ_conc_chan = 0.06, κ_zero = 0.06, κ_miss = 0.05, κ_abs = 0.05, κ_dom = 0.07, κ_scale = 0.02
- Dynamic SE floors per channel/pattern:
  - weak-context concentration_by_channel → se_floor = 0.25–0.35
  - strong-context concentration_by_channel → se_floor = 0.06–0.10

E. Decision-gating (pattern & channel-aware; concrete)
- Constants (initial; sweepable):
  - TOP1_CONC_THRESHOLD = 0.70
  - FEATURE_DOMINANCE_THRESH = 0.60
  - ABS_SPEND_HIGH = 800
  - Z_high = 0.80
  - N_min_zero_samples = 25
  - N_min_conc_by_channel = 25
  - N_min_abs_spend_samples = 50
  - A_high = 0.995
  - SE_accept = 0.06
- Pseudocode:
  - If n_batch == 1:
      If all_zero_flag:
         Use all_zero_context_score & N_zero_samples
         Apply all_zero gating (as earlier)
      Else if concentration_by_channel_flag:
         Use top1_channel_context_score & N_slice_channel
         If top1_channel_context_score ≥ Z_high AND N_slice_channel ≥ N_min_conc_by_channel AND ensemble_agreement ≥ A_high AND GLM_fallback_agrees AND se_combined ≤ SE_accept:
            allow_auto_decision()
         Else:
            priority_audit(gating_reason='concentration_by_channel_stopgap', top1_channel, top1_channel_context_score)
      Else if abs_spend_flag OR feature_dom_fraction_high:
         Apply corresponding gating
      Else:
         normal_gating()
  - For n ≤ 3: relax consensus thresholds slightly (increase SE_accept to 0.08–0.12; require slightly higher N_min)

F. Calibrator & GLM_fallback retrain plan
- Calibrator:
  - Outputs: p10, p50, p90, sd
  - Inputs: model_logit, ensemble_agreement, all_zero_flag, concentration_by_channel_flag, top1_channel, top1_share, spend_entropy_norm, num_nonzero_channels, feature_dom_fraction, missingness_profile, top1_channel_context_score, abs_spend_context_score, CryoSleep, Age_bucket, HomePlanet, Destination, Cabin
  - Loss: quantile loss + ECE penalty + Brier; upweight contradictions (concentration_by_channel transported or abs_spend_high non-transported) ×3–5
  - CV: grouped by fragile flags + top1_channel slices
- GLM_fallback:
  - Explicit interactions: top1_channel × top1_share_bucket × sum_spend_bucket × Age_bucket × HomePlanet × CryoSleep; all_zero × CryoSleep × HomePlanet × Age_bucket
  - Regularization: elastic net; restrain extreme coefficients and enforce sign-stability rules
- Data augmentation:
  - Oversample or upweight rare contradictory samples (concentration_by_channel transported)
  - Synthetic perturbations of spends (winsorize, log1p) to improve robustness
- Shadow-run: ≥14 days; acceptance targets:
  - Reduce contradictions in concentration_by_channel and all_zero_by_ctx slices by ≥30–40%
  - Global ECE within tolerance (≤ +0.5–1.0% absolute)

G. Monitoring, metrics & alerts
- New slice monitors/dashboards:
  - concentration_by_channel_by_ctx: ECE, Brier, FP, FN, contradiction_count, n==1_auto_accept_rate
  - per-channel top1_sign_consistency: track pos_frac for top1_channel slices
  - Global: n==1_auto_accept_contradiction_rate
- Alerts:
  - slice FP/FN > 20% above baseline over 24h → hold auto-accept + alert ops & ML
  - n==1 auto_accept_rate spike → hold gating changes + alert
  - concentration_by_channel contradiction rate > threshold → hold promotions + alert
- Canaries:
  - 0126_01 (all_zero), 0127_01 (high sum_spend), 0133_01 (all_zero), 0133_02 (extreme RoomService), 0134_01 (ShoppingMall concentration)
  - Expected gating: priority_audit for all canaries unless pattern_context_score ≥ Z_high & GLM agrees

H. CI tests & validation
- Unit tests:
  - Records with concentration_by_channel_flag==True and n==1 → gating_reason 'concentration_by_channel_stopgap' unless top1_channel_context_score ≥ Z_high & GLM_fallback_agrees.
  - top1_share is NULL when all_zero_flag==True across components
  - se_combined increases when var_conc_by_channel applies
  - calibrator p10/p90 spread grows for weak-context slices
  - winsorization/log1p for spends applied consistently
- Shadow-run acceptance:
  - contradictions in concentration_by_channel_by_ctx and all_zero_by_ctx reduced ≥30–40%
  - per-slice n==1_auto_accept_contradiction_rate trending down
  - global ECE within ±1% absolute tolerance

I. Operational actions (0–72 hours)
1. Immediate engineering (0–12h):
   - Persist top1_channel, top1_channel_context_score, top1_channel_pos_frac, concentration_by_channel_flag, and other pattern flags in provenance.
   - Enforce n==1 gating: block auto-decision for any record with concentration_by_channel_flag==True OR all_zero_flag OR top1_share≥TOP1_CONC_THRESHOLD OR sum_spend≥ABS_SPEND_HIGH OR feature_dom_fraction≥FEATURE_DOMINANCE_THRESH unless consensus passes.
   - Add canaries including 0134_01 and block auto-decisions for them.
2. Scoring engine (12–48h):
   - Expose var_conc_by_channel, var_feature_dom and se_combined in provenance. Ensure top1_share NULL handling for all_zero and consistent usage downstream.
   - Apply winsorize/log1p transforms for spends, cap per-channel logit contribution (temporary guard).
3. ML pipeline (24–72h):
   - Retrain calibrator & GLM_fallback with new features & interactions; upweight contradictions; start 14+ day shadow validation.
   - Publish updated pooled priors & N_samples per channel-slice daily.
4. Monitoring & ops (24–72h):
   - Deploy dashboards & alerts for concentration_by_channel_by_ctx and canaries.
   - Seed active-label queue with concentration_by_channel candidates for fast labeling.
5. Product/audit (24–72h):
   - Fast-label UI for priority_audit routing.
6. Promotion:
   - Promote only after shadow-run acceptance and canary behavior as expected.

J. Per-record provenance to log (required)
- Raw spends: RoomService, FoodCourt, ShoppingMall, Spa, VRDeck
- sum_spend (raw & log1p), sum_spend_bucket, all_zero_flag, concentration_type
- top1_channel, top1_spend, top1_share (NULL for all_zero), top1_share_bucket
- top1_channel_context_score, top1_channel_pos_frac, top1_channel_N
- spend_entropy_norm, num_nonzero_channels
- missingness_profile, missingness_count
- feature_dom_fraction, feature_dom_channel
- zero_consistency_score, all_zero_context_score, abs_spend_context_score, channel_consistency_score
- N_zero_samples, N_abs_spend_samples, N_conc_by_channel_samples
- var_all_zero, var_missingness, var_concentration, var_abs_spend, var_feature_dom, var_conc_by_channel, var_dispersion
- se_combined
- μ_conc_channel_demo, τ_channel_blend, μ_abs_spend_demo, pooled_prior_snapshot_id
- GLM_fallback_probs, GLM_fallback_agreement_flag
- ensemble_probs, ensemble_agreement
- p10/p50/p90, p_final_sd
- gating_reasons
- scorer_version, pooled_prior_snapshot_id, calibrator_version

K. Hyperparameters (initial; sweepable)
- TOP1_CONC_THRESHOLD = 0.70
- FEATURE_DOMINANCE_THRESH = 0.60
- ABS_SPEND_HIGH = 800 (sweep 600–2500)
- Z_high = 0.80
- N_min_conc_by_channel = 25 (sweep 10–100)
- N_min_abs_spend_samples = 50
- A_high = 0.995
- SE_accept = 0.06 (narrow); increase to 0.08–0.12 for n≤3
- κ_conc_chan = 0.06, κ_zero = 0.06, κ_miss = 0.05, κ_abs = 0.05, κ_dom = 0.07, κ_scale = 0.02
- N0 blending: concentration_by_channel N0 = 25–200 (context dependent), default N0 = 3–10
- per-channel logit cap = 3.0–4.0 logits

L. CI canaries & expected behavior (problem IDs)
- 0126_01 (all_zero, CryoSleep=True, Age 67): expect gating_reason 'all_zero_stopgap', routed to priority_audit unless context strong & GLM agrees.
- 0127_01 (high sum_spend ≈ 1022): expect gating_reason 'abs_spend_or_feature_dom_stopgap'
- 0133_01 (sum_spend==0, CryoSleep=True, Age 19): expect 'all_zero_stopgap'
- 0133_02 (RoomService=2738, top1_share≈1.0): expect 'abs_spend_or_feature_dom_stopgap'
- 0134_01 (ShoppingMall dominance: top1_share≈0.98, sum_spend=744): expect 'concentration_by_channel_stopgap' and routed to priority_audit unless top1_channel_context_score≥Z_high & GLM agrees
- Add historical problem IDs to canary set

M. Quick triage checklist for 0134_01 (immediate debugging)
1. Verify computed fields: sum_spend == 744, all_zero_flag=False, top1_channel==ShoppingMall, top1_spend==730, top1_share≈0.98, spend_entropy_norm low.
2. Check top1_channel_context_score for (ShoppingMall, top1_share_bucket=0.9–1.0, HomePlanet=Earth, Destination=TRAPPIST-1e, Age_bucket=Teen/YoungAdult, CryoSleep=False): how many similar examples? Is context_score low?
3. Inspect μ_conc_channel_demo and blending weight τ_channel; confirm whether pooled prior was available and weighted appropriately for n==1.
4. Inspect se_combined and var_conc_by_channel; confirm whether SE was inflated or left small.
5. Inspect GLM_fallback output & ensemble agreement; did fallback agree? If fallback disagrees and ensemble confidence high → mark for priority audit.
6. If context_score < Z_high OR GLM_fallback_disagrees → priority_audit + active-label queue; add record to canary set and flag for fast-label UI.
7. Run feature attribution (SHAP/local logit contribs) to confirm ShoppingMall was main negative driver; record feature contribution in provenance.

Why these changes will reduce batch errors going forward
- Channel-aware concentration detection prevents blanket concentration rules: top1_share without top1_channel context caused this false negative; using channel-specific priors & variance will reduce sign errors for ShoppingMall-dominant records.
- Pattern-specific pooled priors and larger N0 for fragile slices ensure single-record predictions revert to stable priors when data are insufficient.
- Explicit variance components (var_conc_by_channel, var_feature_dom, etc.) inflate uncertainty for fragile slices and prevent auto-decisions when data insufficient.
- Calibrator & GLM_fallback retrain with interactions and upweighted contradictions improves calibration and fallback strength for channel-concentration slices.
- Provenance, canaries, targeted monitoring and active-labeling accelerate triage and correction loops.

Immediate one-line corrective action
- Enforce n==1 gating: route any record with all_zero_flag == True OR top1_share ≥ 0.70 OR sum_spend ≥ 800 OR feature_dom_fraction ≥ 0.60 to priority_audit unless the pattern_context_score (channel-aware for concentration) ≥ 0.80 AND GLM_fallback/ensemble consensus — add 0126_01, 0127_01, 0133_01, 0133_02 and 0134_01 to canaries and block auto-accept.

Acceptance criteria for promotion (recommended)
- Shadow-run ≥14 days.
- Target: contradictions in all_zero_by_ctx and concentration_by_channel_by_ctx slices reduced by ≥30–40%.
- Global: overall ECE not worsened by >0.5–1.0% absolute.
- No canary should be auto-accepted; canary gating behavior consistent with spec.
- Per-channel sign-consistency drift monitored and within acceptable bounds.

Would you like one of these next steps now?
- I can produce a deterministic scorer skeleton (exact feature calculations + API spec + provenance JSON schema) engineering-ready.
- Or I can produce an exact retrain spec for calibrator & GLM_fallback: dataset selection, upweighting schema, CV folds, hyperparameter sweeps, and acceptance criteria.
- Or I can generate a minimal, immediately-deployable gating patch (code-level pseudocode + CI tests) to implement the n==1 stopgap within 2–12 hours.

Recommendation: for fastest mitigation, I will produce the gating patch (code-level pseudocode + CI tests) now so we can block repeat auto-decisions for concentration_by_channel and all_zero records; follow that with pooled-prior & calibrator retrain spec to fix the root cause. Which would you like me to deliver first?

============================================================